{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KM4 wzglƒôdem KM3 r√≥≈ºni siƒô: alokowaniem z g√≥ry tablic pomocniczych (np. gromadzƒÖcej currentloss), zdeklarowanymi typami danych w definicjach funkcji oraz wykorzystaniem makr @views i @.\n",
    "\n",
    "Czas uczenia notowa≈Çem w nastƒôpujƒÖcych etapach:\n",
    "1) Przed modyfikacjami: 6 epok: 9min 10sek, 3 epoki: 4min 10sek\n",
    "2) Po prealokacji tablic: 6 epok: 9min 4sek, 3 epoki: 4min 5sek\n",
    "3) Po zmianach z pkt.2 oraz dodaniu makr @views i @.: 6 epok: 8min 8sek, 3 epoki: 4min 3sek \n",
    "4) Po zmianach z pkt.2 i pkt.3 oraz deklaracji typ√≥w danych: 6 epok: 7min 51sek, 3 epoki: 3min 54sek\n",
    "\n",
    "O ile dla 3 epok r√≥≈ºnice czasowe nie by≈Çy du≈ºe, tak dla 6 epok uda≈Ço siƒô zredukowaƒá czas oczekiwania, a dla 30 epok zredukowano czas z 2h 45min do 38min."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structures\n",
    "Definition of basic structures for computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty-printing\n",
    "It helps tracking what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 282 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ‚î£‚îÅ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ‚îó‚îÅ ‚àá \");  summary(io, x.gradient)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building\n",
    "At first we have a set of loosely-coupled graph nodes. The following procedures build a proper graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topological_sort (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ‚àà visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ‚àà visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "#resety gradientu sta≈Çej lub gradient√≥w wƒôz≈Ç√≥w na zero\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "#wywolanie forward dla wƒôz≈Ça, wynik to warto≈õƒá wƒôz≈Ça (przejscie w prz√≥d przy alg ro≈ºniczkowania wstecznego)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node) #obliczenie wart. wƒôz≈Ça\n",
    "        reset!(node) #reset poprzedniego gradientu do zera\n",
    "    end\n",
    "    return last(order).output #zwr√≥cenie ostatniej warto≈õci wƒôz≈Ça (szczyt grafu)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "#dla sta≈Çej gradient=nothing, a dla wƒôz≈Ça mamy gradient, gdy jest to pierwsze obliczenie gradientu\n",
    "#lub .+= (dodanie) gradientu do ju≈º posiadanej warto≈õci\n",
    "\n",
    "function backward!(order::Vector; seed=1.0) #przyjmuje wektor posortowanych topologicznie wƒôz≈Ç√≥w\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "#funkcja przechodzi graf w ty≈Ç obliczajƒÖc gradienty, wywo≈ÇujƒÖc funkcje backward\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasted operators\n",
    "The operations act on vectors of values so, the gradients are computed as vector-jacobian-products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: *\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A::Union{Matrix{Float32}, Matrix{Float64}}, x::Union{Vector{Float32}, Vector{Float64}}) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A::Union{Matrix{Float32}, Matrix{Float64}}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = tuple(g * x', A' * g)\n",
    "\n",
    "# x .* y (element-wise multiplication)\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, y::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, y::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    ùüè = ones(length(node.output))\n",
    "    Jx = diagm(y .* ùüè)\n",
    "    Jy = diagm(x .* ùüè)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, y::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = return x .- y\n",
    "backward(::BroadcastedOperator{typeof(-)}, x::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, y::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = tuple(g,-g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, y::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, y::Union{Int32, Int64, Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = tuple(g, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x::Union{Vector{Float32}, Vector{Float64}}) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    ùüè = ones(length(x))\n",
    "    J = ùüè'\n",
    "    tuple(J' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 6 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x::Union{Vector{Float32}, Vector{Float64}}, y::Real) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x::Union{Vector{Float32}, Vector{Float64}}, y::Real, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    ùüè = ones(length(node.output))\n",
    "    Jx = diagm(ùüè ./ y)\n",
    "    Jy = @. (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 7 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: max\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x::Union{Vector{Float32}, Vector{Float64}}, y::Union{Vector{Float32}, Vector{Float64}}) = return max.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(max)}, x::Union{Vector{Float32}, Vector{Float64}}, y::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    Jx = diagm(isless.(y, x))\n",
    "    Jy = diagm(isless.(x, y))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 8 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(exp, x::GraphNode) = BroadcastedOperator(exp, x)\n",
    "forward(::BroadcastedOperator{typeof(exp)}, x::Union{Vector{Float32}, Vector{Float64}}) = return exp.(x)\n",
    "backward(node::BroadcastedOperator{typeof(exp)}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    grad = @. exp.(x) .* g\n",
    "    tuple(grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 9 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(^, x::GraphNode, y::GraphNode) = BroadcastedOperator(^, x, y)\n",
    "forward(::BroadcastedOperator{typeof(^)}, x::Union{Vector{Float32}, Vector{Float64}}, y::Union{Float64, Int64}) = return x .^ y\n",
    "backward(node::BroadcastedOperator{typeof(^)}, x::Union{Vector{Float32}, Vector{Float64}}, y::Union{Float64, Int64}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    ùüè = ones(length(node.output))\n",
    "    Jx = @. y .* (x .^ (y .- ùüè))\n",
    "    Jy = @. (x .^ y) .* log.(abs.(x))\n",
    "    tuple(Jx * g, Jy * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 10 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "softmax(x::GraphNode) = BroadcastedOperator(softmax, x)\n",
    "forward(::BroadcastedOperator{typeof(softmax)}, x::Union{Vector{Float32}, Vector{Float64}}) = return exp.(x) ./ sum(exp.(x))\n",
    "backward(node::BroadcastedOperator{typeof(softmax)}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    y = node.output\n",
    "    J = @. diagm(y) .- y * y'\n",
    "    tuple(J' * g)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's the place where self-made code starts. Starting with 3 activation functions, but only the sigmoid one was used in my both Python's and Julia's models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 11 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear(x::GraphNode) = BroadcastedOperator(linear, x)\n",
    "forward(::BroadcastedOperator{typeof(linear)}, x::Union{Vector{Float32}, Vector{Float64}}) = return x\n",
    "backward(::BroadcastedOperator{typeof(linear)}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = tuple(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 12 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "œÉ(x::GraphNode) = BroadcastedOperator(œÉ, x)\n",
    "forward(::BroadcastedOperator{typeof(œÉ)}, x::Union{Vector{Float32}, Vector{Float64}}) = return 1 ./ (1 .+ exp.(-x))\n",
    "backward(node::BroadcastedOperator{typeof(œÉ)}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    y = node.output\n",
    "    dx = @. g .* y .* (1 .- y)\n",
    "    tuple(dx)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 13 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x::Union{Vector{Float32}, Vector{Float64}}) = return max.(x,0)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x::Union{Vector{Float32}, Vector{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    dx = (x .> 0) .* g\n",
    "    tuple(dx)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants to be used while performing image operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = 28;\n",
    "NCOLS = 28;\n",
    "avgpoolsize = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 14 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv(img::GraphNode, ker::GraphNode) = BroadcastedOperator(conv, img, ker)\n",
    "forward(::BroadcastedOperator{typeof(conv)}, img::Union{Matrix{Float32}, Matrix{Float64}}, ker::Union{Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    PAD = floor(size(ker)[1]/2)\n",
    "    PAD = Int64(PAD)\n",
    "    J = zeros(NROWS,NCOLS) # output size will be equal to input size image\n",
    "    n, m = @. (NROWS,NCOLS) .- PAD # pad is subtracted only on 'one' side. the other is subtracted in for loop below\n",
    "    for i=(PAD+1):n, j=(PAD+1):m\n",
    "        J[i, j] = @views sum(img[(i-PAD):(i+PAD), (j-PAD):(j+PAD)] .* ker) \n",
    "    end\n",
    "    return J\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(conv)}, img::Union{Matrix{Float32}, Matrix{Float64}}, ker::Union{Matrix{Float32}, Matrix{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let\n",
    "    PAD = floor(size(ker)[1]/2)\n",
    "    PAD = Int64(PAD)\n",
    "    dLdimg = conv(g, rot180(ker))\n",
    "    dLdker = conv(rot180(img), g)\n",
    "    tuple(dLdimg[(PAD+1):(end-PAD), (PAD+1):(end-PAD)], dLdker)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All-purpose convolution to be used in backward pass computations of the function above\n",
    "function conv(IMG::Union{Matrix{Float32}, Matrix{Float64}}, KER::Union{Matrix{Float32}, Matrix{Float64}})\n",
    "    PAD = floor(size(KER)[1]/2)\n",
    "    PAD = Int64(PAD)\n",
    "    n, m = size(IMG) .- PAD # pad is subtracted only on 'one' side. the other is subtracted in for loop below\n",
    "    J = zeros(size(IMG)) # output size will be equal to input size image\n",
    "    for i=(PAD+1):n, j=(PAD+1):m\n",
    "        J[i, j] = @views sum(IMG[(i-PAD):(i+PAD), (j-PAD):(j+PAD)] .* KER) \n",
    "    end\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 15 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avgpool(img::GraphNode, ker_size::GraphNode) = BroadcastedOperator(avgpool, img, ker_size)\n",
    "forward(::BroadcastedOperator{typeof(avgpool)}, img::Union{Matrix{Float32}, Matrix{Float64}}, ker_size::Int64) = let\n",
    "    if size(img)[1]%ker_size != 0\n",
    "        error(\"Error with dividing image size into pooling size\")\n",
    "    end\n",
    "    n, m = @. (NROWS,NCOLS) .- ker_size .+ 1\n",
    "    dim = Int64(NROWS / ker_size)\n",
    "    J = zeros(dim,dim)\n",
    "    for i=1:dim, j=1:dim\n",
    "        J[i, j] = @views sum(img[((i-1)*ker_size+1):(i*ker_size), ((j-1)*ker_size+1):(j*ker_size)])/(ker_size^2)\n",
    "    end\n",
    "    return J\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(avgpool)}, img::Union{Matrix{Float32}, Matrix{Float64}}, ker_size::Int64, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let \n",
    "    n = Int64(size(img)[1])\n",
    "    m = n\n",
    "    pooled_n = Int64(size(g)[1])\n",
    "    pooled_m = pooled_n\n",
    "    J = zeros(n, m)\n",
    "        for i=1:pooled_n\n",
    "            for j=1:pooled_m\n",
    "                i_start = (i - 1) * ker_size + 1\n",
    "                i_end = min(i_start + ker_size - 1, n)\n",
    "                j_start = (j - 1) * ker_size + 1\n",
    "                j_end = min(j_start + ker_size - 1, m)\n",
    "                pool_size = (i_end - i_start + 1) * (j_end - j_start + 1)\n",
    "                J[i_start:i_end, j_start:j_end] = @views (g[i, j] * ones(i_end - i_start + 1, j_end - j_start + 1)) / pool_size\n",
    "            end\n",
    "        end\n",
    "    tuple(J)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 16 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flatten(img::GraphNode) = BroadcastedOperator(flatten, img)\n",
    "forward(::BroadcastedOperator{typeof(flatten)}, img::Union{Matrix{Float32}, Matrix{Float64}}) = return reshape(img::Union{Matrix{Float32}, Matrix{Float64}},length(img))\n",
    "backward(::BroadcastedOperator{typeof(flatten)}, img::Union{Matrix{Float32}, Matrix{Float64}}, g::Union{Float32, Float64, Vector{Float32}, Vector{Float64}, Matrix{Float32}, Matrix{Float64}}) = let \n",
    "    J = reshape(g, (Int64(NROWS/avgpoolsize), Int64(NCOLS/avgpoolsize)))\n",
    "    tuple(J)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# load training set\n",
    "trainX = MNIST(split=:train).features;\n",
    "trainX = permutedims(trainX[:,:,:], (2,1,3)); # permutation to ensure that image is not rotated, so the visualization at the end is 'read'able\n",
    "trainY = MNIST(split=:train).targets;\n",
    "\n",
    "# load test set\n",
    "testX = MNIST(split=:test).features;\n",
    "testX = permutedims(testX[:,:,:], (2,1,3));\n",
    "testY = MNIST(split=:test).targets;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "return_prediction (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dense(w, b, x, activation) return activation(w * x .+ b) end\n",
    "function dense(w, x, activation) return activation(w * x) end\n",
    "function dense(w, x) return w * x end\n",
    "\n",
    "function mean_squared_loss(y, ≈∑)\n",
    "    return Constant(0.5) .* (y .- ≈∑) .^ Constant(2)\n",
    "end\n",
    "\n",
    "# Function return_prediction is making some kind of casting an output into a structure that is able to be returned by forward function. It is used in calculating an accuracy of CNN. It does not change the value of the prediction.\n",
    "function return_prediction(y)\n",
    "    return Constant(1) .* (y.-Constant(0))\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function net is for declaring the architecture of the CNN. The arguments are:\n",
    "x = two-dimensional input;\n",
    "wi = fully-connected layers' weights;\n",
    "y = labels;\n",
    "kernel of the convolution layer is passed when convolution function is being called (watch line no. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function net(x, w1, w2, w3, y)\n",
    "    aÃÇ = conv(x, Variable(0.1.*ones(5,5)))\n",
    "    aÃÇ.name = \"aÃÇ\"\n",
    "    bÃÇ = avgpool(aÃÇ, Variable(2))\n",
    "    bÃÇ.name = \"bÃÇ\"\n",
    "    cÃÇ = flatten(bÃÇ)\n",
    "    cÃÇ.name = \"cÃÇ\"\n",
    "    \n",
    "    xÃÇ = dense(w1, cÃÇ, œÉ)\n",
    "    xÃÇ.name = \"xÃÇ\"\n",
    "    zÃÇ = dense(w2, xÃÇ, œÉ)\n",
    "    zÃÇ.name = \"zÃÇ\" \n",
    "    ≈∑ = dense(w3, zÃÇ)\n",
    "    ≈∑.name = \"≈∑\"\n",
    "    E = mean_squared_loss(y, ≈∑)\n",
    "    E.name = \"loss\"\n",
    "    \n",
    "    return topological_sort(E)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights initialization with Xavier Initialization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var w3\n",
       " ‚î£‚îÅ ^ 1√ó10 Matrix{Float64}\n",
       " ‚îó‚îÅ ‚àá Nothing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function xavier_init(in_size::Int64, out_size::Int64)\n",
    "    stddev = sqrt(2/(in_size+out_size))\n",
    "    return randn(Float32, in_size, out_size) * stddev\n",
    "end\n",
    "\n",
    "W1  = Variable(xavier_init(100,196), name=\"w1\")\n",
    "W2  = Variable(xavier_init(10,100), name=\"w2\")\n",
    "W3  = Variable(xavier_init(1,10), name=\"w3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing area for net, forward and backward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Any}:\n",
       " const 0.5\n",
       " var y\n",
       " ‚î£‚îÅ ^ Int64\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " var w3\n",
       " ‚î£‚îÅ ^ 1√ó10 Matrix{Float64}\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " var w2\n",
       " ‚î£‚îÅ ^ 10√ó100 Matrix{Float64}\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " var w1\n",
       " ‚î£‚îÅ ^ 100√ó196 Matrix{Float64}\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " var x\n",
       " ‚î£‚îÅ ^ 28√ó28 Matrix{Float32}\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " var ?\n",
       " ‚î£‚îÅ ^ 5√ó5 Matrix{Float64}\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " op.aÃÇ(typeof(conv))\n",
       " var ?\n",
       " ‚î£‚îÅ ^ Int64\n",
       " ‚îó‚îÅ ‚àá Nothing\n",
       " op.bÃÇ(typeof(avgpool))\n",
       " op.cÃÇ(typeof(flatten))\n",
       " op.?(typeof(mul!))\n",
       " op.xÃÇ(typeof(œÉ))\n",
       " op.?(typeof(mul!))\n",
       " op.zÃÇ(typeof(œÉ))\n",
       " op.≈∑(typeof(mul!))\n",
       " op.?(typeof(-))\n",
       " const 2\n",
       " op.?(typeof(^))\n",
       " op.loss(typeof(*))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(trainX[:,:,65], name=\"x\")\n",
    "y = Variable(trainY[65], name=\"y\")\n",
    "graph = net(x,W1,W2,W3,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 11.094656410584731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forward!(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning loop. The gradients are updated after every single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Current loss: 3.098973922633967\n"
     ]
    }
   ],
   "source": [
    "import Statistics: mean\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 30\n",
    "training_set_size = 60000\n",
    "actual_loss = 0\n",
    "\n",
    "losses = Vector{Float64}(undef, epochs*training_set_size)\n",
    "\n",
    "for i=1:epochs\n",
    "    for j=1:training_set_size\n",
    "        x = Variable(trainX[:,:,j], name=\"x\")\n",
    "        y = Variable(trainY[j], name=\"y\")\n",
    "        graph = net(x, W1, W2, W3, y)\n",
    "        currentloss = forward!(graph)\n",
    "        backward!(graph)\n",
    "\n",
    "        W1.output -= lr*W1.gradient\n",
    "        W2.output -= lr*W2.gradient\n",
    "        W3.output -= lr*W3.gradient\n",
    "\n",
    "        losses[j+((i-1)*training_set_size)] = first(currentloss)\n",
    "    end\n",
    "actual_loss = @views mean(losses[training_set_size*(i-1)+1:training_set_size*i])\n",
    "println(\"Epoch: \", i)\n",
    "println(\"Current loss: \", actual_loss)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's the place where return_prediction is being used. A function predict is able to return an exact prediction of the number. It is needed to implement the same architecture as it was in function 'net'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(x, w1, w2, w3)\n",
    "    aÃÇ = conv(x, Variable(0.1.*ones(5,5)))\n",
    "    aÃÇ.name = \"aÃÇ\"\n",
    "    bÃÇ = avgpool(aÃÇ, Variable(2))\n",
    "    bÃÇ.name = \"bÃÇ\"\n",
    "    cÃÇ = flatten(bÃÇ)\n",
    "    cÃÇ.name = \"cÃÇ\"\n",
    "    \n",
    "    xÃÇ = dense(w1, cÃÇ, œÉ)\n",
    "    xÃÇ.name = \"xÃÇ\"\n",
    "    zÃÇ = dense(w2, xÃÇ, œÉ)\n",
    "    zÃÇ.name = \"zÃÇ\" \n",
    "    ≈∑ = dense(w3, zÃÇ)\n",
    "    ≈∑.name = \"≈∑\"\n",
    "    pred = return_prediction(yÃÇ)\n",
    "    pred.name = \"pred\"\n",
    "    return topological_sort(pred)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's watch some single predictions. Change 'b' constant for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=2240\n",
    "x = Variable(testX[:,:,b], name=\"x\")\n",
    "y = Variable(testY[b], name=\"y\")\n",
    "result = predict(x, W1, W2, W3)\n",
    "exact_prediction = forward!(result) # we can do that thanks to return_prediction\n",
    "println(\"Prediction is: \", Int64(round(first(exact_prediction))))\n",
    "println(\"Label is: \", testY[b])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A loop that calculates accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 10000\n",
    "correct_predictions = 0\n",
    "\n",
    "for i=1:test_set_size\n",
    "    x = Variable(testX[:,:,i], name=\"x\")\n",
    "    result = predict(x, W1, W2, W3)\n",
    "    exact_prediction = forward!(result) # zwraca przybli≈ºonƒÖ warto≈õƒá do zgadywanej, czyli np 1.1 dla 1\n",
    "    if Int64(round(first(exact_prediction))) == testY[i]\n",
    "        correct_predictions += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "accuracy = correct_predictions * 100 / test_set_size\n",
    "println(\"Accuracy is $accuracy %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize a wrong prediction. A loop stops on first wrong prediction and prints it out. To watch some other wrong predictions change the sixth line of the code block below for example to: for i=1000:test_set_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Gadfly: spy\n",
    "#test_set_size = 10000\n",
    "#temp = 1\n",
    "#temp_pred = 0\n",
    "\n",
    "#for i=1:test_set_size\n",
    "#    x = Variable(testX[:,:,i], name=\"x\")\n",
    "#    y = Variable(testY[i], name=\"y\")\n",
    "#    result = predict(x, W1, W2, W3)\n",
    "#    exact_prediction = forward!(result) # zwraca przybli≈ºonƒÖ warto≈õƒá do zgadywanej, czyli np 1.1 dla 1\n",
    "#    if Int64(round(first(exact_prediction))) != testY[i]\n",
    "#        temp = i\n",
    "#        temp_pred = Int64(round(first(exact_prediction)))\n",
    "#        break\n",
    "#    end\n",
    "#end\n",
    "#label = testY[temp]\n",
    "#println(\"Predicted number was labeled as: $label. CNN predicted it was: $temp_pred.\")\n",
    "#b = testX[:,:,temp];\n",
    "#spy(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Gadfly: spy\n",
    "test_set_size = 10000\n",
    "temp = 1\n",
    "temp_pred = 0\n",
    "\n",
    "while true\n",
    "    index = rand(1:test_set_size)\n",
    "    x = Variable(testX[:,:,index], name=\"x\")\n",
    "    y = Variable(testY[index], name=\"y\")\n",
    "    result = predict(x, W1, W2, W3)\n",
    "    exact_prediction = forward!(result) # zwraca przybli≈ºonƒÖ warto≈õƒá do zgadywanej, czyli np 1.1 dla 1\n",
    "    if Int64(round(first(exact_prediction))) != testY[index]\n",
    "        temp = index\n",
    "        temp_pred = Int64(round(first(exact_prediction)))\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "label = testY[temp]\n",
    "println(\"Predicted number was labeled as: $label. CNN predicted it was: $temp_pred.\")\n",
    "b = testX[:,:,temp];\n",
    "spy(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
